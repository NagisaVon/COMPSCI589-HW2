{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPSCI589 Homework 2\n",
    "##### Chang Liu, 3.6.2022\n",
    "\n",
    "## Programming: Multinomial Naive Bayes for Document Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/von/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# utils.py\n",
    "\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"[._;:!`Â¦\\'?,\\\"()\\[\\]]\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "nltk.download('stopwords')  \n",
    "\n",
    "def preprocess_text(text):\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttext = REPLACE_NO_SPACE.sub(\"\", text)\n",
    "\ttext = REPLACE_WITH_SPACE.sub(\" \", text)\n",
    "\ttext = re.sub(r'\\d+', '', text)\n",
    "\ttext = text.lower()\n",
    "\twords = text.split()\n",
    "\treturn [w for w in words if w not in stop_words]\n",
    "\n",
    "def load_training_set(percentage_positives, percentage_negatives):\n",
    "\tvocab = set()\n",
    "\tpositive_instances = []\n",
    "\tnegative_instances = []\n",
    "\tfor filename in glob.glob('train/pos/*.txt'):\n",
    "\t\tif random.random() > percentage_positives:\n",
    "\t\t\tcontinue\n",
    "\t\twith open(os.path.join(os.getcwd(), filename), 'r') as f:\n",
    "\t\t\tcontents = f.read()\n",
    "\t\t\tcontents = preprocess_text(contents)\n",
    "\t\t\tpositive_instances.append(contents)\n",
    "\t\t\tvocab = vocab.union(set(contents))\n",
    "\tfor filename in glob.glob('train/neg/*.txt'):\n",
    "\t\tif random.random() > percentage_negatives:\n",
    "\t\t\tcontinue\n",
    "\t\twith open(os.path.join(os.getcwd(), filename), 'r') as f:\n",
    "\t\t\tcontents = f.read()\n",
    "\t\t\tcontents = preprocess_text(contents)\n",
    "\t\t\tnegative_instances.append(contents)\n",
    "\t\t\tvocab = vocab.union(set(contents))\t\n",
    "\treturn positive_instances, negative_instances, vocab\n",
    "\n",
    "def load_test_set(percentage_positives, percentage_negatives):\n",
    "\tpositive_instances = []\n",
    "\tnegative_instances = []\n",
    "\tfor filename in glob.glob('test/pos/*.txt'):\n",
    "\t\tif random.random() > percentage_positives:\n",
    "\t\t\tcontinue\n",
    "\t\twith open(os.path.join(os.getcwd(), filename), 'r') as f:\n",
    "\t\t\tcontents = f.read()\n",
    "\t\t\tcontents = preprocess_text(contents)\n",
    "\t\t\tpositive_instances.append(contents)\n",
    "\tfor filename in glob.glob('test/neg/*.txt'):\n",
    "\t\tif random.random() > percentage_negatives:\n",
    "\t\t\tcontinue\n",
    "\t\twith open(os.path.join(os.getcwd(), filename), 'r') as f:\n",
    "\t\t\tcontents = f.read()\n",
    "\t\t\tcontents = preprocess_text(contents)\n",
    "\t\t\tnegative_instances.append(contents)\n",
    "\treturn positive_instances, negative_instances\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive training instances: 1274\n",
      "Number of negative training instances: 1228\n",
      "Number of positive test instances: 1270\n",
      "Number of negative test instances: 1252\n",
      "Vocabulary (training set): 29714\n",
      "Prior probability of positive class: 0.5091926458832934\n",
      "Prior probability of negative class: 0.4908073541167066\n",
      "correct Pos Test:  1012\n",
      "correct Neg Test:  1076\n",
      "0.8279143536875495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8279143536875495, 0.4846743295019157)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run.py\n",
    "# from utils import *\n",
    "import pprint\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def naive_bayes(smooth=True, log_likelihood=True):\n",
    "\t\n",
    "\tdebug_v = 0.1 #0.0004\n",
    "\tpercentage_positive_instances_train = debug_v\n",
    "\tpercentage_negative_instances_train = debug_v\n",
    "\n",
    "\tpercentage_positive_instances_test  = debug_v\n",
    "\tpercentage_negative_instances_test  = debug_v\n",
    "\t\n",
    "\t(pos_train, neg_train, vocab) = load_training_set(percentage_positive_instances_train, percentage_negative_instances_train)\n",
    "\t(pos_test,  neg_test)         = load_test_set(percentage_positive_instances_test, percentage_negative_instances_test)\n",
    "\n",
    "\tprint(\"Number of positive training instances:\", len(pos_train))\n",
    "\tprint(\"Number of negative training instances:\", len(neg_train))\n",
    "\tprint(\"Number of positive test instances:\", len(pos_test))\n",
    "\tprint(\"Number of negative test instances:\", len(neg_test))\n",
    "\n",
    "\twith open('vocab.txt','w') as f:\n",
    "\t\tfor word in vocab:\n",
    "\t\t\tf.write(\"%s\\n\" % word)\n",
    "\tprint(\"Vocabulary (training set):\", len(vocab))\n",
    "\n",
    "\tvocab_size = len(vocab)\n",
    "\t# Calculate the prior probabilities\n",
    "\tprior_pos = len(pos_train) / (len(pos_train) + len(neg_train))\n",
    "\tprior_neg = len(neg_train) / (len(pos_train) + len(neg_train))\n",
    "\n",
    "\tprint(\"Prior probability of positive class:\", prior_pos)\n",
    "\tprint(\"Prior probability of negative class:\", prior_neg)\n",
    "\n",
    "\t# Build the likelihoods table\n",
    "\ttrain_dict = {}\n",
    "\tfor word in vocab:\n",
    "\t\ttrain_dict[word] = 0;\n",
    "\t\n",
    "\tlikelihoods = {}\n",
    "\tlikelihoods[\"pos\"] = train_dict.copy()\n",
    "\tlikelihoods[\"pos\"].update( dict(Counter(sum(pos_train, []))) )\n",
    "\tlikelihoods[\"neg\"] = train_dict.copy()\n",
    "\tlikelihoods[\"neg\"].update( dict(Counter(sum(neg_train, []))) )\n",
    "\n",
    "\tword_count_pos = sum(likelihoods[\"pos\"].values()) \n",
    "\tword_count_neg = sum(likelihoods[\"neg\"].values())\n",
    "\n",
    "\tmodel_pos = {}\n",
    "\tmodel_neg = {}\n",
    "\n",
    "\t# calculate probablity, apply lapalce smoothing \n",
    "\tfor word in likelihoods[\"pos\"]:\n",
    "\t\tif smooth:\n",
    "\t\t\tmodel_pos[word] = (likelihoods[\"pos\"][word] + 1) / (word_count_pos + vocab_size) \n",
    "\t\telse:\n",
    "\t\t\tmodel_pos[word] = likelihoods[\"pos\"][word] / word_count_pos\n",
    "\n",
    "\tfor word in likelihoods[\"neg\"]:\n",
    "\t\tif smooth:\n",
    "\t\t\tmodel_neg[word] = (likelihoods[\"neg\"][word] + 1) / (word_count_neg + vocab_size)\n",
    "\t\telse:\n",
    "\t\t\tmodel_neg[word] = likelihoods[\"neg\"][word] / word_count_neg\n",
    "\n",
    "\tpos_test_correct = 0\n",
    "\tfor doc in pos_test:\n",
    "\t\tdoc_dict = dict(Counter(doc))\n",
    "\t\tdoc_p_pos = math.log(prior_pos) if log_likelihood else prior_pos\n",
    "\t\tdoc_p_neg = math.log(prior_neg) if log_likelihood else prior_neg\n",
    "\t\tfor word in doc_dict:\n",
    "\t\t\tif word in model_pos: \n",
    "\t\t\t\t# it should also exist in the negative vacabulary\n",
    "\t\t\t\tif log_likelihood:\n",
    "\t\t\t\t\tdoc_p_pos += math.log(model_pos[word]) if model_pos[word] != 0 else 0\n",
    "\t\t\t\t\tdoc_p_neg += math.log(model_neg[word]) if model_neg[word] != 0 else 0\n",
    "\t\t\t\telse: \n",
    "\t\t\t\t\tdoc_p_pos *= model_pos[word]\n",
    "\t\t\t\t\tdoc_p_neg *= model_neg[word]\n",
    "\t\tif (doc_p_pos > doc_p_neg):\n",
    "\t\t\tpos_test_correct += 1\n",
    "\n",
    "\tneg_test_correct = 0\n",
    "\tfor doc in neg_test:\n",
    "\t\tdoc_dict = dict(Counter(doc))\n",
    "\t\tdoc_p_pos = math.log(prior_pos) if log_likelihood else prior_pos\n",
    "\t\tdoc_p_neg = math.log(prior_neg) if log_likelihood else prior_neg\n",
    "\t\tfor word in doc_dict:\n",
    "\t\t\tif word in model_pos:\n",
    "\t\t\t\t# it should also exist in the negative vacabulary\n",
    "\t\t\t\tif log_likelihood:\n",
    "\t\t\t\t\tdoc_p_pos += math.log(model_pos[word]) if model_pos[word] != 0 else 0\n",
    "\t\t\t\t\tdoc_p_neg += math.log(model_neg[word]) if model_neg[word] != 0 else 0\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdoc_p_pos *= model_pos[word]\n",
    "\t\t\t\t\tdoc_p_neg *= model_neg[word]\n",
    "\t\tif (doc_p_pos < doc_p_neg):\n",
    "\t\t\tneg_test_correct += 1\n",
    "\n",
    "\tprint(\"correct Pos Test: \", pos_test_correct);\n",
    "\tprint(\"correct Neg Test: \", neg_test_correct);\n",
    "\t\n",
    "\taccuracy = (pos_test_correct + neg_test_correct) / (len(pos_test) + len(neg_test))\n",
    "\tprecision = pos_test_correct / (pos_test_correct + neg_test_correct)\n",
    "\tprint(accuracy)\n",
    "\treturn accuracy, precision\n",
    "\n",
    "\n",
    "naive_bayes(smooth=True, log_likelihood=True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
