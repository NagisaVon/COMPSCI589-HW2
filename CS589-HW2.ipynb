{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMPSCI589 Homework 2\n",
    "##### Chang Liu, 3.6.2022\n",
    "\n",
    "## Programming: Multinomial Naive Bayes for Document Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/von/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# utils.py\n",
    "\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"[._;:!`Â¦\\'?,\\\"()\\[\\]]\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "nltk.download('stopwords')  \n",
    "\n",
    "def preprocess_text(text):\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttext = REPLACE_NO_SPACE.sub(\"\", text)\n",
    "\ttext = REPLACE_WITH_SPACE.sub(\" \", text)\n",
    "\ttext = re.sub(r'\\d+', '', text)\n",
    "\ttext = text.lower()\n",
    "\twords = text.split()\n",
    "\treturn [w for w in words if w not in stop_words]\n",
    "\n",
    "def load_training_set(percentage_positives, percentage_negatives):\n",
    "\tvocab = set()\n",
    "\tpositive_instances = []\n",
    "\tnegative_instances = []\n",
    "\tfor filename in glob.glob('train/pos/*.txt'):\n",
    "\t\tif random.random() > percentage_positives:\n",
    "\t\t\tcontinue\n",
    "\t\twith open(os.path.join(os.getcwd(), filename), 'r') as f:\n",
    "\t\t\tcontents = f.read()\n",
    "\t\t\tcontents = preprocess_text(contents)\n",
    "\t\t\tpositive_instances.append(contents)\n",
    "\t\t\tvocab = vocab.union(set(contents))\n",
    "\tfor filename in glob.glob('train/neg/*.txt'):\n",
    "\t\tif random.random() > percentage_negatives:\n",
    "\t\t\tcontinue\n",
    "\t\twith open(os.path.join(os.getcwd(), filename), 'r') as f:\n",
    "\t\t\tcontents = f.read()\n",
    "\t\t\tcontents = preprocess_text(contents)\n",
    "\t\t\tnegative_instances.append(contents)\n",
    "\t\t\tvocab = vocab.union(set(contents))\t\n",
    "\treturn positive_instances, negative_instances, vocab\n",
    "\n",
    "def load_test_set(percentage_positives, percentage_negatives):\n",
    "\tpositive_instances = []\n",
    "\tnegative_instances = []\n",
    "\tfor filename in glob.glob('test/pos/*.txt'):\n",
    "\t\tif random.random() > percentage_positives:\n",
    "\t\t\tcontinue\n",
    "\t\twith open(os.path.join(os.getcwd(), filename), 'r') as f:\n",
    "\t\t\tcontents = f.read()\n",
    "\t\t\tcontents = preprocess_text(contents)\n",
    "\t\t\tpositive_instances.append(contents)\n",
    "\tfor filename in glob.glob('test/neg/*.txt'):\n",
    "\t\tif random.random() > percentage_negatives:\n",
    "\t\t\tcontinue\n",
    "\t\twith open(os.path.join(os.getcwd(), filename), 'r') as f:\n",
    "\t\t\tcontents = f.read()\n",
    "\t\t\tcontents = preprocess_text(contents)\n",
    "\t\t\tnegative_instances.append(contents)\n",
    "\treturn positive_instances, negative_instances\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run.py\n",
    "# from utils import *\n",
    "import pprint\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def naive_bayes(data_percentage, smooth=True, log_likelihood=True):\n",
    "\t\n",
    "\tpercentage_positive_instances_train = data_percentage\n",
    "\tpercentage_negative_instances_train = data_percentage\n",
    "\n",
    "\tpercentage_positive_instances_test  = data_percentage\n",
    "\tpercentage_negative_instances_test  = data_percentage\n",
    "\t\n",
    "\t(pos_train, neg_train, vocab) = load_training_set(percentage_positive_instances_train, percentage_negative_instances_train)\n",
    "\t(pos_test,  neg_test)         = load_test_set(percentage_positive_instances_test, percentage_negative_instances_test)\n",
    "\n",
    "\t# print(\"Number of positive training instances:\", len(pos_train))\n",
    "\t# print(\"Number of negative training instances:\", len(neg_train))\n",
    "\t# print(\"Number of positive test instances:\", len(pos_test))\n",
    "\t# print(\"Number of negative test instances:\", len(neg_test))\n",
    "\n",
    "\twith open('vocab.txt','w') as f:\n",
    "\t\tfor word in vocab:\n",
    "\t\t\tf.write(\"%s\\n\" % word)\n",
    "\t# print(\"Vocabulary (training set):\", len(vocab))\n",
    "\n",
    "\tvocab_size = len(vocab)\n",
    "\t# Calculate the prior probabilities\n",
    "\tprior_pos = len(pos_train) / (len(pos_train) + len(neg_train))\n",
    "\tprior_neg = len(neg_train) / (len(pos_train) + len(neg_train))\n",
    "\n",
    "\t# print(\"Prior probability of positive class:\", prior_pos)\n",
    "\t# print(\"Prior probability of negative class:\", prior_neg)\n",
    "\n",
    "\t# Build the likelihoods table\n",
    "\ttrain_dict = {}\n",
    "\tfor word in vocab:\n",
    "\t\ttrain_dict[word] = 0;\n",
    "\t\n",
    "\tlikelihoods = {}\n",
    "\tlikelihoods[\"pos\"] = train_dict.copy()\n",
    "\tlikelihoods[\"pos\"].update( dict(Counter(sum(pos_train, []))) )\n",
    "\tlikelihoods[\"neg\"] = train_dict.copy()\n",
    "\tlikelihoods[\"neg\"].update( dict(Counter(sum(neg_train, []))) )\n",
    "\n",
    "\tword_count_pos = sum(likelihoods[\"pos\"].values()) \n",
    "\tword_count_neg = sum(likelihoods[\"neg\"].values())\n",
    "\n",
    "\tmodel_pos = {}\n",
    "\tmodel_neg = {}\n",
    "\n",
    "\t# calculate probablity, apply lapalce smoothing \n",
    "\tfor word in likelihoods[\"pos\"]:\n",
    "\t\tif smooth:\n",
    "\t\t\tmodel_pos[word] = (likelihoods[\"pos\"][word] + 1) / (word_count_pos + vocab_size) \n",
    "\t\telse:\n",
    "\t\t\tmodel_pos[word] = likelihoods[\"pos\"][word] / word_count_pos\n",
    "\n",
    "\tfor word in likelihoods[\"neg\"]:\n",
    "\t\tif smooth:\n",
    "\t\t\tmodel_neg[word] = (likelihoods[\"neg\"][word] + 1) / (word_count_neg + vocab_size)\n",
    "\t\telse:\n",
    "\t\t\tmodel_neg[word] = likelihoods[\"neg\"][word] / word_count_neg\n",
    "\n",
    "\tpos_test_correct = 0\n",
    "\tfor doc in pos_test:\n",
    "\t\tdoc_dict = dict(Counter(doc))\n",
    "\t\tdoc_p_pos = math.log(prior_pos) if log_likelihood else prior_pos\n",
    "\t\tdoc_p_neg = math.log(prior_neg) if log_likelihood else prior_neg\n",
    "\t\tfor word in doc_dict:\n",
    "\t\t\tif word in model_pos: \n",
    "\t\t\t\t# it should also exist in the negative vacabulary\n",
    "\t\t\t\tif log_likelihood:\n",
    "\t\t\t\t\tdoc_p_pos += math.log(model_pos[word]) if model_pos[word] != 0 else 0\n",
    "\t\t\t\t\tdoc_p_neg += math.log(model_neg[word]) if model_neg[word] != 0 else 0\n",
    "\t\t\t\telse: \n",
    "\t\t\t\t\tdoc_p_pos *= model_pos[word]\n",
    "\t\t\t\t\tdoc_p_neg *= model_neg[word]\n",
    "\t\tif (doc_p_pos > doc_p_neg):\n",
    "\t\t\tpos_test_correct += 1\n",
    "\n",
    "\tneg_test_correct = 0\n",
    "\tfor doc in neg_test:\n",
    "\t\tdoc_dict = dict(Counter(doc))\n",
    "\t\tdoc_p_pos = math.log(prior_pos) if log_likelihood else prior_pos\n",
    "\t\tdoc_p_neg = math.log(prior_neg) if log_likelihood else prior_neg\n",
    "\t\tfor word in doc_dict:\n",
    "\t\t\tif word in model_pos:\n",
    "\t\t\t\t# it should also exist in the negative vacabulary\n",
    "\t\t\t\tif log_likelihood:\n",
    "\t\t\t\t\tdoc_p_pos += math.log(model_pos[word]) if model_pos[word] != 0 else 0\n",
    "\t\t\t\t\tdoc_p_neg += math.log(model_neg[word]) if model_neg[word] != 0 else 0\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdoc_p_pos *= model_pos[word]\n",
    "\t\t\t\t\tdoc_p_neg *= model_neg[word]\n",
    "\t\tif (doc_p_pos < doc_p_neg):\n",
    "\t\t\tneg_test_correct += 1\n",
    "\n",
    "\t# print(\"correct Pos Test: \", pos_test_correct);\n",
    "\t# print(\"correct Neg Test: \", neg_test_correct);\n",
    "\t\n",
    "\taccuracy = (pos_test_correct + neg_test_correct) / (len(pos_test) + len(neg_test))\n",
    "\tprecision = pos_test_correct / (pos_test_correct + len(neg_test) - neg_test_correct)\n",
    "\trecall = pos_test_correct / len(pos_test)\n",
    "\tconfusion_matrix = [[pos_test_correct, len(pos_test) - pos_test_correct], [len(neg_test) - neg_test_correct, neg_test_correct]]\n",
    "\treturn accuracy, precision, recall, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.1 The Log-transformation trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Without using the log-transformation trick\n",
    "\n",
    "for a new Doc,\n",
    "$$ \n",
    "Pr(y_i | Doc) = Pr(y_i) \\prod_{k=1}^{len(Doc)} Pr(w_k | y_i) \n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| **Accuracy** | **Precision** | **Recall** |\n",
      "| :---: | :---: | :---: |\n",
      "|0.2673583399840383 | 0.26143790849673204 | 0.25559105431309903 |\n",
      "Confusion Matrix:\n",
      "|  | **Predicted +** | **Predicted-** |\n",
      "| :--- | :--- | :--- |\n",
      "| **Actual +** | 640 | 1864 |\n",
      "| **Actual -** | 1808 | 700 |\n"
     ]
    }
   ],
   "source": [
    "(acc, pre, rec, conf) = naive_bayes(0.2, smooth=False, log_likelihood=False)\n",
    "print(\"| **Accuracy** | **Precision** | **Recall** |\")\n",
    "print(\"| :---: | :---: | :---: |\")\n",
    "print(\"|{} | {} | {} |\".format(acc, pre, rec))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"|  | **Predicted +** | **Predicted-** |\")\n",
    "print(\"| :--- | :--- | :--- |\")\n",
    "print(\"| **Actual +** | {} | {} |\".format(conf[0][0], conf[0][1]))\n",
    "print(\"| **Actual -** | {} | {} |\".format(conf[1][0], conf[1][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the log-transformation trick\n",
    "\n",
    "for a new Doc,\n",
    "$$ \n",
    "log(Pr(y_i | Doc)) = log(Pr(y_i)) + \\sum_{k=1}^{len(Doc)} log(Pr(w_k | y_i) )\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| **Accuracy** | **Precision** | **Recall** |\n",
      "| :---: | :---: | :---: |\n",
      "|0.5869565217391305 | 0.5877587758775877 | 0.5266129032258065 |\n",
      "Confusion Matrix:\n",
      "|  | **Predicted +** | **Predicted-** |\n",
      "| :--- | :--- | :--- |\n",
      "| **Actual +** | 1306 | 1174 |\n",
      "| **Actual -** | 916 | 1664 |\n"
     ]
    }
   ],
   "source": [
    "(acc, pre, rec, conf) = naive_bayes(0.2, smooth=False, log_likelihood=True)\n",
    "print(\"| **Accuracy** | **Precision** | **Recall** |\")\n",
    "print(\"| :---: | :---: | :---: |\")\n",
    "print(\"|{} | {} | {} |\".format(acc, pre, rec))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"|  | **Predicted +** | **Predicted-** |\")\n",
    "print(\"| :--- | :--- | :--- |\")\n",
    "print(\"| **Actual +** | {} | {} |\".format(conf[0][0], conf[0][1]))\n",
    "print(\"| **Actual -** | {} | {} |\".format(conf[1][0], conf[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy, precision, recall\n",
    "\n",
    "#### not using the log-transformation trick\n",
    "\n",
    "| **Accuracy** | **Precision** | **Recall** |\n",
    "| :---: | :---: | :---: |\n",
    "|0.2673583399840383 | 0.26143790849673204 | 0.25559105431309903 |\n",
    "\n",
    "#### vs using the trick\n",
    "\n",
    "| **Accuracy** | **Precision** | **Recall** |\n",
    "| :---: | :---: | :---: |\n",
    "|0.5869565217391305 | 0.5877587758775877 | 0.5266129032258065 |\n",
    "\n",
    "\n",
    "#### Confusion Matrix not using the log-transformation trick\n",
    "  \n",
    "|  | **Predicted +** | **Predicted-** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Actual +** | 640 | 1864 |\n",
    "| **Actual -** | 1808 | 700 |\n",
    "\n",
    "#### vs using the trick\n",
    "\n",
    "|  | **Predicted +** | **Predicted-** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Actual +** | 1306 | 1174 |\n",
    "| **Actual -** | 916 | 1664 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Discuss whether classifying instances by computing log-probabilities, instead of probabilities, affects the modelâs performance. Assuming that this transformation does have an impact on performance, does it affect more strongly the modelâs accuracy, precision, or recall? Why do you think that is the case?\n",
    "\n",
    "We can see that using the log-transformation trick does helped a lot with improving the accuracy, the precision, and the recall. I think that's because it solve the problem that the probability could go very low when chaining multiple times, thus brought up the number of true possitive and true negative while lowering the number of false possitive and false negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.2 laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| **Accuracy** | **Precision** | **Recall** |\n",
      "| :---: | :---: | :---: |\n",
      "|0.8204414396500298 | 0.8489991296779809 | 0.7782209812524931 |\n",
      "Confusion Matrix:\n",
      "|  | **Predicted +** | **Predicted-** |\n",
      "| :--- | :--- | :--- |\n",
      "| **Actual +** | 1951 | 556 |\n",
      "| **Actual -** | 347 | 2175 |\n"
     ]
    }
   ],
   "source": [
    "(acc, pre, rec, conf) = naive_bayes(0.2, smooth=True, log_likelihood=True)\n",
    "print(\"| **Accuracy** | **Precision** | **Recall** |\")\n",
    "print(\"| :---: | :---: | :---: |\")\n",
    "print(\"|{} | {} | {} |\".format(acc, pre, rec))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"|  | **Predicted +** | **Predicted-** |\")\n",
    "print(\"| :--- | :--- | :--- |\")\n",
    "print(\"| **Actual +** | {} | {} |\".format(conf[0][0], conf[0][1]))\n",
    "print(\"| **Actual -** | {} | {} |\".format(conf[1][0], conf[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date with laplace smoothing, while $\\alpha= 1$ \n",
    "\n",
    "| **Accuracy** | **Precision** | **Recall** |\n",
    "| :---: | :---: | :---: |\n",
    "|0.8204414396500298 | 0.8489991296779809 | 0.7782209812524931 |\n",
    "\n",
    "Confusion Matrix:\n",
    "|  | **Predicted +** | **Predicted-** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Actual +** | 1951 | 556 |\n",
    "| **Actual -** | 347 | 2175 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "def naive_bayes_smoothing_plot(data_percentage):\n",
    "\t\n",
    "\tpercentage_positive_instances_train = data_percentage\n",
    "\tpercentage_negative_instances_train = data_percentage\n",
    "\n",
    "\tpercentage_positive_instances_test  = data_percentage\n",
    "\tpercentage_negative_instances_test  = data_percentage\n",
    "\t\n",
    "\t(pos_train, neg_train, vocab) = load_training_set(percentage_positive_instances_train, percentage_negative_instances_train)\n",
    "\t(pos_test,  neg_test)         = load_test_set(percentage_positive_instances_test, percentage_negative_instances_test)\n",
    "\n",
    "\talphas = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "\taccuracies = [ deploy_nb_with_smoothing(pos_train, neg_train, vocab, pos_test, neg_test, alpha) for alpha in alphas]\n",
    "\treturn alphas, accuracies\n",
    "\n",
    "def deploy_nb_with_smoothing(pos_train, neg_train, vocab, pos_test, neg_test, alpha):\n",
    "\n",
    "\tvocab_size = len(vocab)\n",
    "\t# Calculate the prior probabilities\n",
    "\tprior_pos = len(pos_train) / (len(pos_train) + len(neg_train))\n",
    "\tprior_neg = len(neg_train) / (len(pos_train) + len(neg_train))\n",
    "\n",
    "\n",
    "\ttrain_dict = {}\n",
    "\tfor word in vocab:\n",
    "\t\ttrain_dict[word] = 0;\n",
    "\t\n",
    "\tlikelihoods = {}\n",
    "\tlikelihoods[\"pos\"] = train_dict.copy()\n",
    "\tlikelihoods[\"pos\"].update( dict(Counter(sum(pos_train, []))) )\n",
    "\tlikelihoods[\"neg\"] = train_dict.copy()\n",
    "\tlikelihoods[\"neg\"].update( dict(Counter(sum(neg_train, []))) )\n",
    "\n",
    "\tword_count_pos = sum(likelihoods[\"pos\"].values()) \n",
    "\tword_count_neg = sum(likelihoods[\"neg\"].values())\n",
    "\n",
    "\tmodel_pos = {}\n",
    "\tmodel_neg = {}\n",
    "\n",
    "\t# calculate probablity, apply lapalce smoothing \n",
    "\tfor word in likelihoods[\"pos\"]:\n",
    "\t\tmodel_pos[word] = (likelihoods[\"pos\"][word] + alpha) / (word_count_pos + vocab_size*alpha) \n",
    "\n",
    "\tfor word in likelihoods[\"neg\"]:\n",
    "\t\tmodel_neg[word] = (likelihoods[\"neg\"][word] + alpha) / (word_count_neg + vocab_size*alpha)\n",
    "\n",
    "\tpos_test_correct = 0\n",
    "\tfor doc in pos_test:\n",
    "\t\tdoc_dict = dict(Counter(doc))\n",
    "\t\tdoc_p_pos = math.log(prior_pos) \n",
    "\t\tdoc_p_neg = math.log(prior_neg) \n",
    "\t\tfor word in doc_dict:\n",
    "\t\t\tif word in model_pos: \n",
    "\t\t\t\t# it should also exist in the negative vacabulary\n",
    "\t\t\t\tdoc_p_pos += math.log(model_pos[word]) if model_pos[word] != 0 else 0\n",
    "\t\t\t\tdoc_p_neg += math.log(model_neg[word]) if model_neg[word] != 0 else 0\n",
    "\t\tif (doc_p_pos > doc_p_neg):\n",
    "\t\t\tpos_test_correct += 1\n",
    "\n",
    "\tneg_test_correct = 0\n",
    "\tfor doc in neg_test:\n",
    "\t\tdoc_dict = dict(Counter(doc))\n",
    "\t\tdoc_p_pos = math.log(prior_pos) \n",
    "\t\tdoc_p_neg = math.log(prior_neg) \n",
    "\t\tfor word in doc_dict:\n",
    "\t\t\tif word in model_pos:\n",
    "\t\t\t\t# it should also exist in the negative vacabulary\n",
    "\t\t\t\tdoc_p_pos += math.log(model_pos[word]) if model_pos[word] != 0 else 0\n",
    "\t\t\t\tdoc_p_neg += math.log(model_neg[word]) if model_neg[word] != 0 else 0\n",
    "\t\tif (doc_p_pos < doc_p_neg):\n",
    "\t\t\tneg_test_correct += 1\n",
    "\n",
    "\taccuracy = (pos_test_correct + neg_test_correct) / (len(pos_test) + len(neg_test))\n",
    "\treturn accuracy\n",
    "\n",
    "(alphas, accuracies) = naive_bayes_smoothing_plot(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAq80lEQVR4nO3dd3hUZd7G8e8vBQIIoYMQqnRQWgjoq2tXdFXsUoXQ7Oyq6+rqrnXXdd1Vd3VVUJAqzY6uYi+rUhIICKFIkRKaoYQWQtrz/pHBjXEIE8jkzGTuz3Xlck6buRnD3Jxz5jzHnHOIiIiUFOV1ABERCU0qCBER8UsFISIifqkgRETELxWEiIj4pYIQERG/YrwOUF7q16/vWrZs6XUMEZGwsmjRop3OuQb+llWagmjZsiWpqalexxARCStmtvFoy3SISURE/FJBiIiIXyoIERHxSwUhIiJ+qSBERMQvFYSIiPilghCRoNuadYh1mQe8jiFlVGmugxCR0HMot4AXv1zH2C/XkV9QyLAzWnHXRe04qao+esKB/i+JSLlzzvFh+nYee28lW7IOcXnXJtSMi2Hitz/wn2VbeejyzlzSpTFm5nVUKYUKQkTK1dofD/DIu+n8d81O2jeqyYxRfTj9lHoAXNczgQfeWs6try7m7HYNeLRfZ1rUq+FxYjkaqyy3HE1MTHQaakPEOwcO5/Psp2t45esfqFYlmrsubMeQPi2Iif75qc78gkImz9vI0x+tJr/Qcfu5bRh9dmuqxkR7lDyymdki51yi32UqCBE5Ec453lmylcffX8mP+w9zfWICv+/bgfonVS11u+17c3j0vXTeX7ad1g1q8Ocru3DGKfUrKLUcoYIQkaBI37qXh+ekk7JhD6clxPPIFZ3p3rxOmZ7j89U/8uA7y9m8+xBXdW/K/Zd2pEHN0stFyo8KQkTKVVZ2Lk999D2vLthI7epV+P3F7bk+sRlRUcd30jknr4DnP1/L2C/XUS02mt/37cDApObH/XwSOBWEiJSLgkLH7NTNPDl3FXsP5TGkTwvuurA98dVjy+X51/54gD++vYz563fTtVlt/nJlF7o0jS+X5xb/SiuIoF4oZ2Z9zWy1ma01s/v8LG9uZp+bWZqZfWdml/pZfsDMfhfMnCJybIs37eHK57/hD28uo23Dmrx3x1k80q9LuZUDQJuGJzFjVB+euaErGbuzueLfX/Pouys4cDi/3F5DAhe0PQgziwa+By4EMoAUYIBzbkWxdV4C0pxzL5pZJ+B951zLYstfBxywwDn3j9JeT3sQIsGRuf8wT85dxWuLMmhUqyr3X9qRK7o2Cfo1DHuz83jyw1VMX7iJhjWr6tqJICltDyKY10EkAWudc+t9IWYC/YAVxdZxQC3f43hg65EFZnYl8ANwMIgZReQo8goKmTpvI898/D05+QXcdHZr7jivbYVdBR1fPZa/XHUq1+raCc8E8/90U2BzsekMoHeJdR4GPjKzO4AawAUAZnYScC9Fex9HPbxkZqOB0QDNmzcvr9wiEe/bdTt5eE463+84wFlt6/PQ5Z1p0/AkT7J0b16HObf/30/XTlz0zFe6dqKCeD1Y3wBgknMuAbgUmGpmURQVxzPOuVJH93LOveScS3TOJTZo4Pee2yJSBtv2HuL26YsZ+PICsnMLGDekJ1OGJ3lWDkfEREcx4sxWfHr3OZzfsSFPffw9l/zrv3y7bqenuSq7YO5BbAGaFZtO8M0rbgTQF8A5N8/M4oD6FO1pXGtmTwK1gUIzy3HO/TuIeUUi1uH8Asb/9wf+/dlaCp3jtxe05eazTyEuNrT+hd44Po4XBvX86dqJgS8v0LUTQRTMgkgB2ppZK4qKoT8wsMQ6m4DzgUlm1hGIAzKdc2cdWcHMHgYOqBxEguPzVT/yyLvpbNiVzcWdG/HHX3eiWd3qXscq1bntG/LxnWf/dO3Epyt36NqJIAjaISbnXD5wO/AhsBKY7ZxLN7NHzewK32p3A6PMbCkwAxjmKsuFGSIhbuOug4ycnELypBSizJg8PIlxQxJDvhyOiIuN5u6L2vPBb35Fpya1+OPby7nqxW9ZvmWv19EqDV0oJxJhDuUW8MIXaxn31Xpioowx57dl+P+1okqM16ckj59zjreXbOHP761kT3au7jtRBl59zVVEQohzjg+Wb+cv/ym6R0O/bk34wyUdaRwf53W0E2ZmXNU9gfPaN+LJD1fpvhPlRHsQIhFgzY79PPxuOt+s3UWHxjV55IrO9G5dz+tYQZO2aQ8PvLWcFdv26dqJY9BYTCIRan9OHs9+uoaJ32ygepWiY/aDejf/xT0aKiPddyIwKgiRCFNY6HgrbQtPzF3FzgOHuSGxGfdc3J56x7hHQ2Wk+06UTgUhEkGWb9nLQ3PSWbRxD12b1ebRKzrTtVltr2N5Tved8E8FIRIB9hzM5R8frWb6wk3UrV6Fey/pwLU9EnRdQDG678QvqSBEKrGCQsfMlE38/cPV7M/JZ0ifFtx5YTviq5XfMNyVje478T8qCJFKatHGPTw0ZznLt+yjd6u6PNKvMx0a1zr2hqJrJ3x0HYRIJZOdm8+D76Tzuu8eDc8O6M7lp52s7/uXga6dODbtQYiEmb2H8hg+KYW0TXsY/atTuOO8NtSIsH/1BkPJayeevr5rRHzry7NbjopI+dp54DADXprPdxlZvDCoB/dd0kHlUE6O3HfiT5d1Yv76XQyduJB9OXlex/KUCkIkTGzJOsT1Y+exfucBJgztRd8uJ3sdqdI5ct+JsYN7smrbfkZOSuVQboHXsTyjghAJA+szD3Ddi9+SeeAw00b05lftdIOsYDq3Q0OeuaEbKRt3c8uri8jNL/Q6kidUECIhbsXWfVw/bh6H8wuZOboPiS3reh0pIlzetQmPX3UqX6zO5K7ZSygorBzna8tCBy9FQtiijbtJnpjCSVVjmDqyN6c08PbWn5FmQFJz9h3K468frKJmXCyPX9Ulor7dpIIQCVH/XZPJ6CmLaBwfx7SRvWlau5rXkSLSTWefwt5DebzwxTpqVYvhD5d09DpShVFBiISgucu3M2ZGGq0b1GDqiN4aM8hj91zcnn05eYz7cj3x1WK59Zw2XkeqECoIkRDz+qIMfv/6Uro1q83EYUnEV9eQGV4zMx69ogv7c/J5cu5qasXFMrhPC69jBZ0KQiSETPrmBx5+dwVntqnPuCE9dY1DCImKMv5xXVcO5OTzp3eWUzMuhn7dmnodK6j0LSaREOCc47lP1/Dwuyu4uHMjJgxLVDmEoNjoKJ4f1IOklnW5e/ZSPl25w+tIQaWCEPGYc47H31/JUx9/z9U9mvL8wB6661kIi4uNZvzQRDo1qcWtry5m/vpdXkcKGhWEiIcKCh33vbGMl//7A8POaMk/ru0aEbcDDXc142KZlJxEs7rVGTk5le8ysryOFBT6TRTxSG5+IWNmpDErdTNjzmvDQ5d3iugb14SbujWqMG1Eb2pXj2XoKwtZs2O/15HKnQpCxAOHcgsYPTWV/yzbxgOXduSui9pH1AVYlUXj+DimjehNdFQUQyYsZPPubK8jlaugFoSZ9TWz1Wa21szu87O8uZl9bmZpZvadmV3qm3+hmS0ys2W+/54XzJwiFWlfTh5DX1nIl99n8sTVpzLqV629jiQnoGX9GkwbmcShvAIGT1jAj/tyvI5UboJWEGYWDTwPXAJ0AgaYWacSq/0RmO2c6w70B17wzd8JXO6cOxUYCkwNVk6RirTrwGEGvjyfxZv28Gz/7vRPau51JCkHHRrXYmJyLzL3H+bGVxaSlZ3rdaRyEcw9iCRgrXNuvXMuF5gJ9CuxjgOO3B8xHtgK4JxLc85t9c1PB6qZmS4llbC2be8hrh83jzU7DvDyjYlc3rWJ15GkHPVoXoeXhiSyPvMgyZNSOHg43+tIJyyYBdEU2FxsOsM3r7iHgcFmlgG8D9zh53muARY75w4HI6RIRdiw8yDXvjiPHfsOM2V4Eud2aOh1JAmCM9vW59kB3Vm6OYvRU1M5nB/e95Lw+iT1AGCScy4BuBSYamY/ZTKzzsDfgJv8bWxmo80s1cxSMzMzKySwSFmt2r6P68bNIzs3nxmj+tC7dT2vI0kQ9e3SmCev7co3a3cxZkYa+QXhey+JYBbEFqBZsekE37ziRgCzAZxz84A4oD6AmSUAbwE3OufW+XsB59xLzrlE51xigwa6gYqEnrRNe7hh3HyiDGbfdDqnJsR7HUkqwLU9E3jwsk58mL6De99YRmGY3ksimAWRArQ1s1ZmVoWik9BzSqyzCTgfwMw6UlQQmWZWG/gPcJ9z7psgZhQJmm/X7mTQ+AXUrh7L6zefQdtGNb2OJBVo+Jmt+O0FbXljcQaPvrcC58KvJII22ItzLt/Mbgc+BKKBV5xz6Wb2KJDqnJsD3A28bGZ3UnTCephzzvm2awM8aGYP+p7yIufcj8HKK1KePkrfzu0z0mhVrwZTRyTRsFac15HEA785vy37DuXzyjc/EF8tljsvbOd1pDKxcGw1fxITE11qaqrXMUR4Ky2D3732HV2axjM5uRe1q1fxOpJ4qLDQ8fs3vuP1RRk8eFknhp/ZyutIP2Nmi5xzif6WabhIkXI0dd4G/vROOqe3rsfLQxM5SSOyRryoKOOJq0/lQE4+j763gppxMVyX2OzYG4YAr7/FJFJpPP/5Wv70TjoXdGzIxOReKgf5SUx0FP8a0I2z2tbn3je+Y+7ybV5HCogKQuQEOed44oNV/P3D1VzZrQkvDu5JXKyG65afqxoTzdjBPenarDZjZizhv2tC/6v5KgiRE1BQ6Hjg7eWM/XIdg/s05+nruxGr4brlKGpUjWHSsCRaN6jB6CmLWLRxj9eRSqXfZJHjlFdQyJ2zljB9wSZuOecUHuvXRcN1yzHFV49lyogkGtWqSvLEhazcts/rSEelghA5Djl5Bdw0dRFzlm7l933bc2/fDhquWwLWsGYcU0f0pnqVGIZMWMgPOw96HckvFYRIGR04nM+wiQv5fPWP/PnKLtx6ThuvI0kYala3OtNGJlHoHIPHL2Db3kNeR/oFFYRIGew5mMugl+eTsmEP/7yhG4P7tPA6koSxNg1rMjk5ib2H8hg8fgG7DoTWmKQqCJEA7diXw/Xj5rFy+37GDe5Jv24lBycWKbtTE+IZPzSRjD2HGDYxhf05eV5H+okKQiQAm3Zlc+3Yb9madYhJyb24oFMjryNJJdKndT1eHNyDldv2MWJyKjl5oTFMuApC5Bi+37Gfa8d+y/6cfF4d1YczTqnvdSSphM7r0Iinru9Kyobd3PrqYvJCYJhwFYRIKb7LyOKGcfMAmDX6dLo1q+1tIKnU+nVryp+v7MJnq37krtlLKfB4mHCNBSByFPPX72Lk5FTq1Ihl2ojetKhXw+tIEgEG9W7B3kN5PDl3NTXjYvjLlV08+wq1CkLEj89W7eCWaYuLvoo4ojeN4zVct1ScW89pw75D+Yz9ch3x1WK5t28HT3KoIERKeGfJFu6evZSOJ9di8vAk6tbQcN1S8e7t2559OXm8+MU6asXFcss5p1R4BhWESDHTF2zigbeX0atlXSYMTaRmXKzXkSRCmRmP9evC/px8/jZ3FbWqxTCod8Ved6OCEPGZ8PUPPPbeCs5t30AjskpIiI4ynr6+Kwdy8vjj28upGRfLFV2bVNjr61tMIsDrizJ47L0V9O3cmHFDElUOEjJio6N4YVBPerWsy12zlvDZqh0V9toqCIl4n6zYwb1vfMeZberzrwHdqBKjvxYSWqpViWbC0EQ6nFyTW6YtZsH6XRXyuvqbIBEtZcNubpu+mM5NajF2SE+qxmjPQUJTzbhYJicnkVCnGiMmp7IsY2/QX1MFIRFr5bZ9DJ+UQtPa1Zg4TLcIldBX76SqTBvZm/hqsQyduJC1P+4P6uupICQibd6dzY2vLKRGlRimjEii3klVvY4kEpCT46sxbWRvoswYPH4hm3dnB+21VBAScTL3H2bIhAXk5hcyZUQSCXWqex1JpExa1a/B1BFJZOfmM2TCAn7cnxOU11FBSETZl5PHsIkL2b4vh1eG9aJdo5peRxI5Lh1PrsXE5CR27DvM8EkpQRm3SQddJWLk5BUwekoqq7fv5+WhifRsUcfrSCInpGeLOrx0Y08O5xUSHYT7oQd1D8LM+prZajNba2b3+Vne3Mw+N7M0M/vOzC4ttuwPvu1Wm9nFwcwplV9BoeM3M9OYv343/7iuK+e2b+h1JJFycVbbBkG7P0nQ9iDMLBp4HrgQyABSzGyOc25FsdX+CMx2zr1oZp2A94GWvsf9gc5AE+ATM2vnnAuNu2hIWHHO8cBby/gwfQcPXtaJK7vrTnAigQjmHkQSsNY5t945lwvMBPqVWMcBtXyP44Gtvsf9gJnOucPOuR+Atb7nEymzf3y0mpkpm7n93DYMP7OV13FEwkYwC6IpsLnYdIZvXnEPA4PNLIOivYc7yrAtZjbazFLNLDUzM7O8ckslMuHrH3j+83UMSGrG3Re18zqOSFgJqCDM7E0z+7WZlXehDAAmOecSgEuBqWV5DefcS865ROdcYoMGDco5moS7t9L+N77Sn6881bObroiEq0A/jF8ABgJrzOwJM2sfwDZbgGbFphN884obAcwGcM7NA+KA+gFuK3JUn6/6kXte+47TW9fjn/27BeUbHiKVXUAF4Zz7xDk3COgBbKDopPG3ZpZsZkcbMD8FaGtmrcysCkUnneeUWGcTcD6AmXWkqCAyfev1N7OqZtYKaAssLNsfTSLVoo27ueXVRbRvXJOXbtSw3SLHK+DDOWZWDxgGjATSgH9RVBgf+1vfOZcP3A58CKyk6NtK6Wb2qJld4VvtbmCUmS0FZgDDXJF0ivYsVgBzgdv0DSYJxOrt+0memELjWnFMSk7SDX9EToA5d+yr78zsLaA9MJWicwbbii1Ldc4lBi9iYBITE11qaqrXMcRDGXuyuebFb3EO3rjlDJrV1RAaIsdiZouO9hke6HUQzzrnPve3IBTKQWTXgcPcOGEhh3ILmH3z6SoHkXIQ6CGmTmZW+8iEmdUxs1uDE0mkbA4czmfYxBS2ZB1iwrBedGhc69gbicgxBVoQo5xzWUcmnHN7gFFBSSRSBofzC7hpaiortu3jhUE96NWyrteRRCqNQAsi2op9idw3jEaV4EQSCUxBoePOWUv4Zu0unrzmNM7vGJzxaEQiVaDnIOYCs8xsnG/6Jt88EU845/jTO8t5f9l2Hri0I9f0TPA6kkilE2hB3EtRKdzim/4YGB+URCIBeOaTNUxfsImbzz6FUb9q7XUckUopoIJwzhUCL/p+RDw16ZsfePbTNVyfmMC9fQO5qF9EjkdABWFmbYG/Ap0outoZAOec/ukmFeqdJVt4+N0VXNipEY9fpfGVRIIp0JPUEynae8gHzgWmANOCFUrEny+/z+Tu2UtJalWX5wZ0JyZad8wVCaZA/4ZVc859StGV1xudcw8Dvw5eLJGfS9u0h5unLqJto5qMH5qo8ZVEKkCgJ6kP+4bhXmNmt1M0supJwYsl8j9rf9xP8qQUGtSsyuThvail8ZVEKkSgexC/AaoDY4CewGBgaLBCiRyxNesQQyYsJCYqiqkjkmhYM+7YG4lIuTjmHoTvorgbnHO/Aw4AyUFPJQLsPpjLkAkLOJCTz8yb+tCiXg2vI4lElGPuQfiG2T6zArKI/OTg4XySJ6Wwec8hxg9NpHOTeK8jiUScQM9BpJnZHOA14OCRmc65N4OSSiJabn4hN09bxLKMLMYO7knv1vW8jiQSkQItiDhgF3BesXkOUEFIuSosdNw1ewn/XbOTJ685jYs6N/Y6kkjECvRKap13kKBzzvHIu+m899027rukA9f3anbsjUQkaAK9knoiRXsMP+OcG17uiSRiPfvpWibP28ios1pxk8ZXEvFcoIeY3iv2OA64Ctha/nEkUk2dv5FnPvmeq3s05Q+XdNQQGiIhINBDTG8UnzazGcDXQUkkEec/323jwXeWc36HhvztmtOIilI5iISC4x3Mpi3QsDyDSGT6es1OfjsrjZ7N6/DvgT2I1fhKIiEj0HMQ+/n5OYjtFN0jQuS4Ld2cxeipqZzS4CQmDO1FtSoaX0kklAR6iKlmsINIZFmXeYDkSSnUrVGFycOTiK+u8ZVEQk1A+/NmdpWZxRebrm1mVwYtlVRq2/Ye4sYJC4kymDqiN41qaXwlkVAU6AHfh5xze49MOOeygIeCkkgqtazsXG6csJC9h/KYlJxEq/oaX0kkVAVaEP7WC2Sgv75mttrM1prZfX6WP2NmS3w/35tZVrFlT5pZupmtNLNnTd97DHvZufkMn5TCxl3ZvHRjT7o01fhKIqEs0OsgUs3saeB53/RtwKLSNvCNAvs8cCGQAaSY2Rzn3Ioj6zjn7iy2/h1Ad9/jM4D/A07zLf4aOBv4IsC8EmLyCgq59dXFLNmcxQuDenDGKfW9jiQixxDoHsQdQC4wC5gJ5FBUEqVJAtY659Y753J92/UrZf0BwAzfY0fRBXlVgKpALLAjwKwSYgoLHfe8tpQvVmfyl6tOpW+Xk72OJCIBCPRbTAeBXxwiOoamwOZi0xlAb38rmlkLoBXwme/15pnZ58A2wIB/O+dW+tluNDAaoHnz5mWMJxXBOcdj/1nB20u2cs/F7RmQpP9PIuEi0G8xfWxmtYtN1zGzD8sxR3/gdd+9JzCzNkBHIIGiojnPzM4quZFz7iXnXKJzLrFBgwblGEfKywtfrGPiNxtI/r+W3HrOKV7HEZEyCPQQU33fN5cAcM7t4dhXUm8Big/HmeCb509//nd4CYrGeprvnDvgnDsAfACcHmBWCRFjv1zH3z9czZXdmvCnX3fS+EoiYSbQgig0s5+ODZhZS/yM7lpCCtDWzFqZWRWKSmBOyZXMrANQB5hXbPYm4GwzizGzWIpOUP/iEJOEJuccz3z8PU98sIrLTjuZv1/XVeMriYShQL/F9ADwtZl9SdE5gbPwHfs/GudcvpndDnwIRAOvOOfSzexRINU5d6Qs+gMznXPFC+d1im5OtIyiIprrnHs30D+UeMc5xxMfrGLcV+u5tmcCf7vmNKJVDiJhyX7+uVzKimYNKSqFNKAa8KNz7qsgZiuTxMREl5qa6nWMiFZY6Hj43XSmzNvI4D7NefSKLtpzEAlxZrbIOZfob1mgg/WNBH5D0XmEJUAfig4JnVfKZhJBCgod97+5jFmpmxl5Zise+LXu6SAS7gI9B/EboBew0Tl3LkUXtGUFK5SEl/yCQu6avYRZqZsZc14blYNIJRHoOYgc51yOmWFmVZ1zq8ysfVCTSVjIzS9kzIw05qZv556L23PbuW28jiQi5STQgsjwXQfxNvCxme0BNgYrlISHnLwCbpm2iM9XZ/LgZZ0YfmYrryOJSDkK9Erqq3wPH/Zd4RwPzA1aKgl52bn5jJycyrz1u3j8qlMZ2FtXSItUNoHuQfzEOfdlMIJI+Nifk0fyxBQWb9rDU9d15eoeCV5HEpEgKHNBSGTLys5l6CsLSd+6j+cG9ODXp2ngPZHKSgUhAdt54DCDxy9gfeZBxg7uyQWdGnkdSUSCSAUhAdmxL4eBL89nS9YhJgxL5Ky2GhxRpLJTQcgxZezJZtD4Bezcf5jJyUn0bl3P60giUgFUEFKqDTsPMmj8Avbn5DFtZG+6N6/jdSQRqSAqCDmqNTv2M2j8AvIKCpk+qo/uIS0SYVQQ4lf61r0MmbCQKDNm3XQ67RrV9DqSiFSwQMdikgiyZHMWA16aT9WYKGbf1EflIBKhtAchP5OyYTfJE1OoUyOW6SP70Kxuda8jiYhHVBDyk6/X7GTUlFROrh3H9JF9aBwf53UkEfGQDjEJAJ+t2sHwySm0qFedWaNPVzmIiPYgBD5Yto0xM9Po0LgWU4YnUadGFa8jiUgIUEFEuLfTtnD3a0vp1qw2E5N7USsu1utIIhIidIgpgs1cuIk7Zy+hV8s6TBmepHIQkZ/RHkSEmvTNDzz87grObteAcUN6Ehcb7XUkEQkxKogINPbLdTzxwSou6tSI5wZ2p2qMykFEfkkFEUGcc/zzkzX869M1XN61CU9f35XYaB1lFBH/VBARwjnHEx+sYtxX67m2ZwJ/u+Y0oqPM61giEsKC+s9HM+trZqvNbK2Z3edn+TNmtsT3872ZZRVb1tzMPjKzlWa2wsxaBjNrZVZY6HhoTjrjvlrP4D7NeVLlICIBCNoehJlFA88DFwIZQIqZzXHOrTiyjnPuzmLr3wF0L/YUU4C/OOc+NrOTgMJgZa3MCgod97+5jFmpmxl1Vivuv7QjZioHETm2YO5BJAFrnXPrnXO5wEygXynrDwBmAJhZJyDGOfcxgHPugHMuO4hZK6X8gkLumr2EWambGXNeG5WDiJRJMAuiKbC52HSGb94vmFkLoBXwmW9WOyDLzN40szQz+7tvj0QClJtfyO3T03hnyVbuubg9d13UXuUgImUSKl9h6Q+87pwr8E3HAGcBvwN6Aa2BYSU3MrPRZpZqZqmZmZkVlTXk5eQVcNPUVOamb+fByzpx27ltvI4kImEomAWxBWhWbDrBN8+f/vgOL/lkAEt8h6fygbeBHiU3cs695JxLdM4lNmjQoHxSh7ns3HyGT0rhi+8zefyqUxl+ZiuvI4lImApmQaQAbc2slZlVoagE5pRcycw6AHWAeSW2rW1mRz71zwNWlNxWfm5/Th43TljI/PW7eOq6rgzs3dzrSCISxoJWEL5/+d8OfAisBGY759LN7FEzu6LYqv2Bmc45V2zbAooOL31qZssAA14OVtbKICs7l8HjF7BkcxbPDejB1T0SvI4kImHOin0uh7XExESXmprqdQxP7DxwmMHjF7A+8yAvDOrBBZ0aeR1JRMKEmS1yziX6W6YrqcPcjn05DHx5PluyDjFhWCJntdW5GBEpHyqIMJaxJ5tB4xewc/9hJicn0bt1Pa8jiUglooIIUxt2HmTQ+AXsz8lj2sjedG9ex+tIIlLJqCDC0Jod+xk0fgH5hY7po/rQpWm815FEpBJSQYSZFVv3MXjCAqKjjJmj+9CuUU2vI4lIJRUqV1JLAJZszqL/S/OoGhPF7JtOVzmISFBpDyJMpGzYTfLEFOrUiGX6yD40q1vd60giUsmpIMLA12t2MmpKKifXjmP6yD40jo/zOpKIRAAdYgpxn63awfDJKbSoV51Zo09XOYhIhdEeRAj7YNk2xsxMo0PjWkwZnkSdGlW8jiQiEUQFEaLeTtvC3a8tpVuz2kxM7kWtuFivI4lIhNEhphA0c+Em7py9hF4t6zBleJLKQUQ8oT2IEDPpmx94+N0VnN2uAeOG9CQuVjfSExFvqCBCyNgv1/HEB6u4qFMjnhvYnaoxKgcR8Y4KIgQ45/jnJ2v416druLxrE56+viux0Tr6JyLeUkF4zDnHE3NXMe7L9VzbM4G/XXMa0VHmdSwRERWElwoLHY+8m87keRsZ0qcFj1zRmSiVg4iECBWERwoKHfe/uYxZqZsZdVYr7r+0I2YqBxEJHSoID+QXFHL3a0t5Z8lWxpzXhjsvbKdyEJGQo4KoYLn5hYyZkcbc9O3cc3F7bju3jdeRRET8UkFUoJy8Am6ZtojPV2fy4GWdGH5mK68jiYgclQqigmTn5jNycirz1u/i8atOZWDv5l5HEhEplQqiAuzPySN5YgqLN+3hqeu6cnWPBK8jiYgckwoiyLKycxn6ykLSt+7juQE9+PVpJ3sdSUQkICqIINp54DCDxy9gfeZBxg7uyQWdGnkdSUQkYEEdz8HM+prZajNba2b3+Vn+jJkt8f18b2ZZJZbXMrMMM/t3MHMGw459Odwwbh4bdh1kwrBElYOIhJ2g7UGYWTTwPHAhkAGkmNkc59yKI+s45+4stv4dQPcST/MY8FWwMgZLxp5sBo1fwM79h5mcnETv1vW8jiQiUmbB3INIAtY659Y753KBmUC/UtYfAMw4MmFmPYFGwEdBzFjuNuw8yA3j5rPnYC7TRvZWOYhI2ApmQTQFNhebzvDN+wUzawG0Aj7zTUcBTwG/K+0FzGy0maWaWWpmZma5hD4Ra3/cz/Xj5pGdm8/0UX3o3ryO15FERI5bqIwp3R943TlX4Ju+FXjfOZdR2kbOuZecc4nOucQGDRoEPWRpVmzdxw3j5uOAWTedTpem8Z7mERE5UcH8FtMWoFmx6QTfPH/6A7cVmz4dOMvMbgVOAqqY2QHn3C9OdIeCJZuzuHHCAk6qGsOro/rQqn4NryOJiJywYBZECtDWzFpRVAz9gYElVzKzDkAdYN6Rec65QcWWDwMSQ7UcUjbsJnliCnVqxDJ9ZB+a1a3udSQRkXIRtENMzrl84HbgQ2AlMNs5l25mj5rZFcVW7Q/MdM65YGUJlm/W7uTGCQtpWKsqr910hspBRCoVC8PPZb8SExNdampqhb3eZ6t2cPO0xbSuX4OpI3rToGbVCnttEZHyYmaLnHOJ/pbpSurj8MGybYyZmUaHxrWYMjyJOjWqeB1JRKTcqSDK6O20Ldz92lK6NavNxORe1IqL9TqSiEhQhMrXXMPCrJRN3Dl7Cb1a1mHK8CSVg4hUatqDCNDkbzfw0Jx0zm7XgHFDehIXG+11JBGRoFJBBGDsl+t44oNVXNSpEc8N7E7VGJWDiFR+KohSOOf45ydr+Nena7i8axOevr4rsdE6KicikUEFcRTOOZ6Yu4pxX67n2p4J/O2a04iOMq9jiYhUGBWEH4WFjkfeTWfyvI0M7tOcR6/oQpTKQUQijAqihIJCx/1vLmNW6mZGndWK+y/tiJnKQUQijwqimPyCQu5+bSnvLNnKmPPacOeF7VQOIhKxVBA+ufmFjJmRxtz07dxzcXtuO7eN15FERDylggBy8gq4ZdoiPl+dyYOXdWL4ma28jiQi4rmIL4js3HxGTUnl23W7ePyqUxnYu7nXkUREQkLEF0RWdh6bdmfz1HVdubpHgtdxRERCRsQXRJPa1fj4zrM1dIaISAm6LBhUDiIifqggRETELxWEiIj4pYIQERG/VBAiIuKXCkJERPxSQYiIiF8qCBER8cucc15nKBdmlgls9E3GA3tLeexvXn1gZxlftvjzBLqs5PyjTZeWu7yzHm35seaF03sbaG69t5XvvQ0keyS/ty2ccw38ruGcq3Q/wEulPT7KvNQTeZ1Al5Wcf7Tp0nKXd9ajLT/WvHB6bwPNrfe28r23gWTXe+v/p7IeYnr3GI+PtvxEXifQZSXnH236WLnL6ljb+lt+rHnh9N6WJXdZ6b0t/bHX720g2fXe+lFpDjGdKDNLdc4lep0jEOGUFcIrbzhlhfDKG05ZIbzyBitrZd2DOB4veR2gDMIpK4RX3nDKCuGVN5yyQnjlDUpW7UGIiIhf2oMQERG/VBAiIuKXCkJERPxSQQTAzGqYWaqZXeZ1lmMxs45mNtbMXjezW7zOUxozu9LMXjazWWZ2kdd5jsXMWpvZBDN73ess/vh+Tyf73tNBXuc5llB/P4sLw9/V8vkcKOvFFeH0A7wC/AgsLzG/L7AaWAvcF8DzPAr8HrgsHPL6tokCpoVJ1jrAhDB6b18PZtbjzQ0MAS73PZ5VURlP9H2uyPezHLIG/Xe1nPOe0OdAhf8hK/gN/RXQo/gbCkQD64DWQBVgKdAJOBV4r8RPQ+BCoD8wrAIK4oTz+ra5AvgAGBjqWX3bPQX0CIf31rddRRZEWXL/AejmW2d6RWU83rxevJ/lkDXov6vllbc8PgdiqMScc1+ZWcsSs5OAtc659QBmNhPo55z7K/CLQ0hmdg5Qg6K/gIfM7H3nXGGo5vU9zxxgjpn9B5geqlnNzIAngA+cc4uDkbM883qhLLmBDCABWIJHh4/LmHdFBcf7mbJkNbOVVNDv6tGU9b0tj8+BSDwH0RTYXGw6wzfPL+fcA86531L0Br8crHIoRZnymtk5ZvasmY0D3g92uBLKlBW4A7gAuNbMbg5msKMo63tbz8zGAt3N7A/BDleKo+V+E7jGzF7kxIZgKG9+84bQ+1nc0d5br39Xj+Zo7225fA5U6j2I8uScm+R1hkA4574AvvA4RkCcc88Cz3qdI1DOuV1AKH04/Ixz7iCQ7HWOQIX6+1lcGP6ufkE5fA5E4h7EFqBZsekE37xQFU55wykrhF/eI8ItdzjlDaesEOS8kVgQKUBbM2tlZlUoOgE9x+NMpQmnvOGUFcIv7xHhljuc8oZTVgh2Xi/OxlfgWf8ZwDYgj6JjcyN88y8Fvqfo7P8DXucMx7zhlDUc84Zr7nDKG05ZvcqrwfpERMSvSDzEJCIiAVBBiIiIXyoIERHxSwUhIiJ+qSBERMQvFYSIiPilghApJ2a2wczqn+g6IqFCBSEiIn6pIESOg5m9bWaLzCzdzEaXWNbSzFaZ2atmttJ3V6/qxVa5w8wWm9kyM+vg2ybJzOaZWZqZfWtm7Sv0DyTihwpC5PgMd871BBKBMWZWr8Ty9sALzrmOwD7g1mLLdjrnegAvAr/zzVsFnOWc6w48CDwe1PQiAVBBiByfMWa2FJhP0WiabUss3+yc+8b3eBpwZrFlb/r+uwho6XscD7xmZsuBZ4DOwQgtUhYqCJEy8t1l8ALgdOdcVyANiCuxWslBzopPH/b9t4D/3ZPlMeBz51wX4HI/zydS4VQQImUXD+xxzmX7ziH08bNOczM73fd4IPB1AM95ZBz/YeWSUuQEqSBEym4uEFPsPsXz/ayzGrjNt04dis43lOZJ4K9mlobu9CghQsN9i5Qz343l3/MdLhIJW9qDEBERv7QHISIifmkPQkRE/FJBiIiIXyoIERHxSwUhIiJ+qSBERMQvFYSIiPj1/1dmF1a1Sk1vAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(alphas, accuracies)\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Analyze this graph and discuss why do you think the accuracy suffers when Î± is too high or too low.\n",
    "\n",
    "The graph reaches a peak between $\\alpha=10^1$ and $\\alpha=10^2$. The accuracy suffers when $\\alpha$ is too low or too high because $\\alpha$ become either too dominant or have too little effect in the fraction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.3 Whole training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually q_3_4\n",
    "def naive_bayes_q_4_5(percentage_train, smooth_alpha, log_likelihood=True):\n",
    "    \n",
    "\tpercentage_positive_instances_train = percentage_train\n",
    "\tpercentage_negative_instances_train = percentage_train\n",
    "\n",
    "\tpercentage_positive_instances_test  = 1\n",
    "\tpercentage_negative_instances_test  = 1\n",
    "\t\n",
    "\t(pos_train, neg_train, vocab) = load_training_set(percentage_positive_instances_train, percentage_negative_instances_train)\n",
    "\t(pos_test,  neg_test)         = load_test_set(percentage_positive_instances_test, percentage_negative_instances_test)\n",
    "\n",
    "\t# print(\"Number of positive training instances:\", len(pos_train))\n",
    "\t# print(\"Number of negative training instances:\", len(neg_train))\n",
    "\t# print(\"Number of positive test instances:\", len(pos_test))\n",
    "\t# print(\"Number of negative test instances:\", len(neg_test))\n",
    "\n",
    "\twith open('vocab.txt','w') as f:\n",
    "\t\tfor word in vocab:\n",
    "\t\t\tf.write(\"%s\\n\" % word)\n",
    "\t# print(\"Vocabulary (training set):\", len(vocab))\n",
    "\n",
    "\tvocab_size = len(vocab)\n",
    "\t# Calculate the prior probabilities\n",
    "\tprior_pos = len(pos_train) / (len(pos_train) + len(neg_train))\n",
    "\tprior_neg = len(neg_train) / (len(pos_train) + len(neg_train))\n",
    "\n",
    "\t# print(\"Prior probability of positive class:\", prior_pos)\n",
    "\t# print(\"Prior probability of negative class:\", prior_neg)\n",
    "\n",
    "\t# Build the likelihoods table\n",
    "\ttrain_dict = {}\n",
    "\tfor word in vocab:\n",
    "\t\ttrain_dict[word] = 0;\n",
    "\t\n",
    "\tlikelihoods = {}\n",
    "\tlikelihoods[\"pos\"] = train_dict.copy()\n",
    "\tlikelihoods[\"pos\"].update( dict(Counter(sum(pos_train, []))) )\n",
    "\tlikelihoods[\"neg\"] = train_dict.copy()\n",
    "\tlikelihoods[\"neg\"].update( dict(Counter(sum(neg_train, []))) )\n",
    "\n",
    "\tword_count_pos = sum(likelihoods[\"pos\"].values()) \n",
    "\tword_count_neg = sum(likelihoods[\"neg\"].values())\n",
    "\n",
    "\tmodel_pos = {}\n",
    "\tmodel_neg = {}\n",
    "\n",
    "\t# calculate probablity, apply lapalce smoothing \n",
    "\tfor word in likelihoods[\"pos\"]:\n",
    "\t\tmodel_pos[word] = (likelihoods[\"pos\"][word] + smooth_alpha) / (word_count_pos + vocab_size*smooth_alpha) \n",
    "\n",
    "\tfor word in likelihoods[\"neg\"]:\n",
    "\t\tmodel_neg[word] = (likelihoods[\"neg\"][word] + smooth_alpha) / (word_count_neg + vocab_size*smooth_alpha)\n",
    "\n",
    "\tpos_test_correct = 0\n",
    "\tfor doc in pos_test:\n",
    "\t\tdoc_dict = dict(Counter(doc))\n",
    "\t\tdoc_p_pos = math.log(prior_pos) if log_likelihood else prior_pos\n",
    "\t\tdoc_p_neg = math.log(prior_neg) if log_likelihood else prior_neg\n",
    "\t\tfor word in doc_dict:\n",
    "\t\t\tif word in model_pos: \n",
    "\t\t\t\t# it should also exist in the negative vacabulary\n",
    "\t\t\t\tif log_likelihood:\n",
    "\t\t\t\t\tdoc_p_pos += math.log(model_pos[word]) if model_pos[word] != 0 else 0\n",
    "\t\t\t\t\tdoc_p_neg += math.log(model_neg[word]) if model_neg[word] != 0 else 0\n",
    "\t\t\t\telse: \n",
    "\t\t\t\t\tdoc_p_pos *= model_pos[word]\n",
    "\t\t\t\t\tdoc_p_neg *= model_neg[word]\n",
    "\t\tif (doc_p_pos > doc_p_neg):\n",
    "\t\t\tpos_test_correct += 1\n",
    "\n",
    "\tneg_test_correct = 0\n",
    "\tfor doc in neg_test:\n",
    "\t\tdoc_dict = dict(Counter(doc))\n",
    "\t\tdoc_p_pos = math.log(prior_pos) if log_likelihood else prior_pos\n",
    "\t\tdoc_p_neg = math.log(prior_neg) if log_likelihood else prior_neg\n",
    "\t\tfor word in doc_dict:\n",
    "\t\t\tif word in model_pos:\n",
    "\t\t\t\t# it should also exist in the negative vacabulary\n",
    "\t\t\t\tif log_likelihood:\n",
    "\t\t\t\t\tdoc_p_pos += math.log(model_pos[word]) if model_pos[word] != 0 else 0\n",
    "\t\t\t\t\tdoc_p_neg += math.log(model_neg[word]) if model_neg[word] != 0 else 0\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdoc_p_pos *= model_pos[word]\n",
    "\t\t\t\t\tdoc_p_neg *= model_neg[word]\n",
    "\t\tif (doc_p_pos < doc_p_neg):\n",
    "\t\t\tneg_test_correct += 1\n",
    "\n",
    "\t# print(\"correct Pos Test: \", pos_test_correct);\n",
    "\t# print(\"correct Neg Test: \", neg_test_correct);\n",
    "\t\n",
    "\taccuracy = (pos_test_correct + neg_test_correct) / (len(pos_test) + len(neg_test))\n",
    "\tprecision = pos_test_correct / (pos_test_correct + len(neg_test) - neg_test_correct)\n",
    "\trecall = pos_test_correct / len(pos_test)\n",
    "\tconfusion_matrix = [[pos_test_correct, len(pos_test) - pos_test_correct], [len(neg_test) - neg_test_correct, neg_test_correct]]\n",
    "\treturn accuracy, precision, recall, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| **Accuracy** | **Precision** | **Recall** |\n",
      "| :---: | :---: | :---: |\n",
      "|0.84248 | 0.8748030117317458 | 0.79936 |\n",
      "Confusion Matrix:\n",
      "|  | **Predicted +** | **Predicted-** |\n",
      "| :--- | :--- | :--- |\n",
      "| **Actual +** | 9992 | 2508 |\n",
      "| **Actual -** | 1430 | 11070 |\n"
     ]
    }
   ],
   "source": [
    "(acc, pre, rec, conf) = naive_bayes_q_4_5(1, 10, log_likelihood=True)\n",
    "print(\"| **Accuracy** | **Precision** | **Recall** |\")\n",
    "print(\"| :---: | :---: | :---: |\")\n",
    "print(\"|{} | {} | {} |\".format(acc, pre, rec))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"|  | **Predicted +** | **Predicted-** |\")\n",
    "print(\"| :--- | :--- | :--- |\")\n",
    "print(\"| **Actual +** | {} | {} |\".format(conf[0][0], conf[0][1]))\n",
    "print(\"| **Actual -** | {} | {} |\".format(conf[1][0], conf[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data when using the whole train dataset \n",
    "\n",
    "| **Accuracy** | **Precision** | **Recall** |\n",
    "| :---: | :---: | :---: |\n",
    "|0.84248 | 0.8748030117317458 | 0.79936 |\n",
    "\n",
    "Confusion Matrix:\n",
    "|  | **Predicted +** | **Predicted-** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Actual +** | 9992 | 2508 |\n",
    "| **Actual -** | 1430 | 11070 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.4 Half training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| **Accuracy** | **Precision** | **Recall** |\n",
      "| :---: | :---: | :---: |\n",
      "|0.8276 | 0.8685868586858686 | 0.772 |\n",
      "Confusion Matrix:\n",
      "|  | **Predicted +** | **Predicted-** |\n",
      "| :--- | :--- | :--- |\n",
      "| **Actual +** | 9650 | 2850 |\n",
      "| **Actual -** | 1460 | 11040 |\n"
     ]
    }
   ],
   "source": [
    "(acc, pre, rec, conf) = naive_bayes_q_4_5(0.5, 10, log_likelihood=True)\n",
    "print(\"| **Accuracy** | **Precision** | **Recall** |\")\n",
    "print(\"| :---: | :---: | :---: |\")\n",
    "print(\"|{} | {} | {} |\".format(acc, pre, rec))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(\"|  | **Predicted +** | **Predicted-** |\")\n",
    "print(\"| :--- | :--- | :--- |\")\n",
    "print(\"| **Actual +** | {} | {} |\".format(conf[0][0], conf[0][1]))\n",
    "print(\"| **Actual -** | {} | {} |\".format(conf[1][0], conf[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data when using half the train dataset \n",
    "\n",
    "| **Accuracy** | **Precision** | **Recall** |\n",
    "| :---: | :---: | :---: |\n",
    "|0.8436 | 0.8736082115518441 | 0.80344 |\n",
    "\n",
    "Confusion Matrix:\n",
    "|  | **Predicted +** | **Predicted-** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Actual +** | 10043 | 2457 |\n",
    "| **Actual -** | 1453 | 11047 |\n",
    "   \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Discuss whether using such a smaller training set had any impact on the performance your learned model. Analyze the confusion matrices (of this question and the previous one) and discuss whether one particular class was more affected by changing the size of the training set.\n",
    "\n",
    "It didn't make a big difference. I also tried to vary the $\\alpha$ value, which also didn't make a big difference. I think the reason behind is that after a certain point, the precision will no long benefit from the increased size of the training set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.5 accuracy vs precision vs recall\n",
    "\n",
    "> In this application (i.e., accurately classifying movie reviews), would you say that it is more important to have high accuracy, high precision, or high recall? Justify your opinion.\n",
    "\n",
    "I think it depends on how we want to use the model. For example, if we want to hide all the negative reviews, we should try to lower the rate of false positive, even though that might increase the rate of false negative, which means that we might also hide some of the positive reviews. In that case, we should try to optimize for precision.\n",
    "\n",
    "$$ precision = \\frac{true\\;positive}{true\\;positive + false\\;positive} $$ \n",
    "\n",
    "In other case, if we want to modify other recommandation system, or push ads to users that give out positive reviews, we should try to avoid false negative since we do not want to lose a potential user. In that case, we should try to optimize for recall.\n",
    "\n",
    "$$ recall = \\frac{true\\;positive}{true\\;positive + false\\;negative} $$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q.6 Unbalanced training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actually q_3_4\n",
    "def naive_bayes_unbalance(percentage_positive_instances_train, percentage_negative_instances_train, smooth_alpha, log_likelihood=True):\n",
    "    \n",
    "\tpercentage_positive_instances_test  = 1\n",
    "\tpercentage_negative_instances_test  = 1\n",
    "\t\n",
    "\t(pos_train, neg_train, vocab) = load_training_set(percentage_positive_instances_train, percentage_negative_instances_train)\n",
    "\t(pos_test,  neg_test)         = load_test_set(percentage_positive_instances_test, percentage_negative_instances_test)\n",
    "\n",
    "\t# print(\"Number of positive training instances:\", len(pos_train))\n",
    "\t# print(\"Number of negative training instances:\", len(neg_train))\n",
    "\t# print(\"Number of positive test instances:\", len(pos_test))\n",
    "\t# print(\"Number of negative test instances:\", len(neg_test))\n",
    "\n",
    "\twith open('vocab.txt','w') as f:\n",
    "\t\tfor word in vocab:\n",
    "\t\t\tf.write(\"%s\\n\" % word)\n",
    "\t# print(\"Vocabulary (training set):\", len(vocab))\n",
    "\n",
    "\tvocab_size = len(vocab)\n",
    "\t# Calculate the prior probabilities\n",
    "\tprior_pos = len(pos_train) / (len(pos_train) + len(neg_train))\n",
    "\tprior_neg = len(neg_train) / (len(pos_train) + len(neg_train))\n",
    "\n",
    "\t# print(\"Prior probability of positive class:\", prior_pos)\n",
    "\t# print(\"Prior probability of negative class:\", prior_neg)\n",
    "\n",
    "\t# Build the likelihoods table\n",
    "\ttrain_dict = {}\n",
    "\tfor word in vocab:\n",
    "\t\ttrain_dict[word] = 0;\n",
    "\t\n",
    "\tlikelihoods = {}\n",
    "\tlikelihoods[\"pos\"] = train_dict.copy()\n",
    "\tlikelihoods[\"pos\"].update( dict(Counter(sum(pos_train, []))) )\n",
    "\tlikelihoods[\"neg\"] = train_dict.copy()\n",
    "\tlikelihoods[\"neg\"].update( dict(Counter(sum(neg_train, []))) )\n",
    "\n",
    "\tword_count_pos = sum(likelihoods[\"pos\"].values()) \n",
    "\tword_count_neg = sum(likelihoods[\"neg\"].values())\n",
    "\n",
    "\tmodel_pos = {}\n",
    "\tmodel_neg = {}\n",
    "\n",
    "\t# calculate probablity, apply lapalce smoothing \n",
    "\tfor word in likelihoods[\"pos\"]:\n",
    "\t\tmodel_pos[word] = (likelihoods[\"pos\"][word] + smooth_alpha) / (word_count_pos + vocab_size*smooth_alpha) \n",
    "\n",
    "\tfor word in likelihoods[\"neg\"]:\n",
    "\t\tmodel_neg[word] = (likelihoods[\"neg\"][word] + smooth_alpha) / (word_count_neg + vocab_size*smooth_alpha)\n",
    "\n",
    "\tpos_test_correct = 0\n",
    "\tfor doc in pos_test:\n",
    "\t\tdoc_dict = dict(Counter(doc))\n",
    "\t\tdoc_p_pos = math.log(prior_pos) if log_likelihood else prior_pos\n",
    "\t\tdoc_p_neg = math.log(prior_neg) if log_likelihood else prior_neg\n",
    "\t\tfor word in doc_dict:\n",
    "\t\t\tif word in model_pos: \n",
    "\t\t\t\t# it should also exist in the negative vacabulary\n",
    "\t\t\t\tif log_likelihood:\n",
    "\t\t\t\t\tdoc_p_pos += math.log(model_pos[word]) if model_pos[word] != 0 else 0\n",
    "\t\t\t\t\tdoc_p_neg += math.log(model_neg[word]) if model_neg[word] != 0 else 0\n",
    "\t\t\t\telse: \n",
    "\t\t\t\t\tdoc_p_pos *= model_pos[word]\n",
    "\t\t\t\t\tdoc_p_neg *= model_neg[word]\n",
    "\t\tif (doc_p_pos > doc_p_neg):\n",
    "\t\t\tpos_test_correct += 1\n",
    "\n",
    "\tneg_test_correct = 0\n",
    "\tfor doc in neg_test:\n",
    "\t\tdoc_dict = dict(Counter(doc))\n",
    "\t\tdoc_p_pos = math.log(prior_pos) if log_likelihood else prior_pos\n",
    "\t\tdoc_p_neg = math.log(prior_neg) if log_likelihood else prior_neg\n",
    "\t\tfor word in doc_dict:\n",
    "\t\t\tif word in model_pos:\n",
    "\t\t\t\t# it should also exist in the negative vacabulary\n",
    "\t\t\t\tif log_likelihood:\n",
    "\t\t\t\t\tdoc_p_pos += math.log(model_pos[word]) if model_pos[word] != 0 else 0\n",
    "\t\t\t\t\tdoc_p_neg += math.log(model_neg[word]) if model_neg[word] != 0 else 0\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tdoc_p_pos *= model_pos[word]\n",
    "\t\t\t\t\tdoc_p_neg *= model_neg[word]\n",
    "\t\tif (doc_p_pos < doc_p_neg):\n",
    "\t\t\tneg_test_correct += 1\n",
    "\n",
    "\t# print(\"correct Pos Test: \", pos_test_correct);\n",
    "\t# print(\"correct Neg Test: \", neg_test_correct);\n",
    "\t\n",
    "\taccuracy = (pos_test_correct + neg_test_correct) / (len(pos_test) + len(neg_test))\n",
    "\tprecision = pos_test_correct / (pos_test_correct + len(neg_test) - neg_test_correct)\n",
    "\trecall = pos_test_correct / len(pos_test)\n",
    "\tconfusion_matrix = [[pos_test_correct, len(pos_test) - pos_test_correct], [len(neg_test) - neg_test_correct, neg_test_correct]]\n",
    "\treturn accuracy, precision, recall, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| **Accuracy** | **Precision** | **Recall** |\n",
      "| :---: | :---: | :---: |\n",
      "|0.60748 | 0.9459010952538998 | 0.228 |\n",
      "Confusion Matrix:\n",
      "|  | **Predicted +** | **Predicted-** |\n",
      "| :--- | :--- | :--- |\n",
      "| **Actual +** | 2850 | 9650 |\n",
      "| **Actual -** | 163 | 12337 |\n"
     ]
    }
   ],
   "source": [
    "(acc, pre, rec, conf) = naive_bayes_unbalance(0.1, 0.5, 1, log_likelihood=True)\n",
    "print(\"| **Accuracy** | **Precision** | **Recall** |\")\n",
    "print(\"| :---: | :---: | :---: |\")\n",
    "print(\"|{} | {} | {} |\".format(acc, pre, rec))\n",
    "print(\"Confusion Matrix:\") \n",
    "print(\"|  | **Predicted +** | **Predicted-** |\")\n",
    "print(\"| :--- | :--- | :--- |\")\n",
    "print(\"| **Actual +** | {} | {} |\".format(conf[0][0], conf[0][1]))\n",
    "print(\"| **Actual -** | {} | {} |\".format(conf[1][0], conf[1][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Accuracy** | **Precision** | **Recall** |\n",
    "| :---: | :---: | :---: |\n",
    "|0.60748 | 0.9459010952538998 | 0.228 |\n",
    "\n",
    "Confusion Matrix:\n",
    "|  | **Predicted +** | **Predicted-** |\n",
    "| :--- | :--- | :--- |\n",
    "| **Actual +** | 2850 | 9650 |\n",
    "| **Actual -** | 163 | 12337 |      \n",
    "     \n",
    "### Discussion\n",
    "\n",
    "> Discuss how training under an unbalanced dataset affected each of these performance metrics.\n",
    "\n",
    "An unbalanced dataset will completely destroy the performance of the model since the model will be biased towards the majority class. At $alpha = 10$, the model actually classifies all the reviews as negitive, and precision = 0. I changed $alpha$ to 1, and the model still performs better than the balanced train set, but is better thanÂ $alpha = 10$. I think this is because the one class of the training set has only a small amount of data, and the lapace smoothing will smooth all the features of the data."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
